services:
  identifier:
    image: semtech/mu-identifier:1.10.1
    environment:
      SESSION_COOKIE_SECURE: "on"
      DEFAULT_MU_AUTH_ALLOWED_GROUPS_HEADER: '[{"variables":[],"name":"public"},{"variables":[],"name":"clean"}]'
    restart: always
  dispatcher:
    image: semtech/mu-dispatcher:2.1.0-beta.2
    links:
      - resource:resource
    volumes:
      - ./config/dispatcher:/config
  database:
    image: tenforce/virtuoso:1.3.2-virtuoso7.2.5.1
    environment:
      SPARQL_UPDATE: "true"
      DEFAULT_GRAPH: "http://mu.semte.ch/application"
    volumes:
      - ./data/db:/data
      - ./config/virtuoso/virtuoso.ini:/data/virtuoso.ini
    restart: always
  migrations:
    image: semtech/mu-migrations-service:0.9.0
    volumes:
      - ./config/migrations:/data/migrations
    restart: always
  resource:
    image: semtech/mu-cl-resources:1.23.0
    environment:
      CACHE_CLEAR_PATH: "http://cache/.mu/clear-keys"
    links:
      - database:database
    volumes:
      - ./config/resources:/config
    restart: always
  frontend:
    image: lblod/frontend-ai-hackathon-group-1:latest # In real life, it would be a fixed version
    restart: always

  ###
  # The chat UI is the gradio web interface that enables quick and easy access to interacitons with LLMs
  ###
  chat_ui:
    image: lblod/abb-wave-1-group-1-chat-ui-service:latest
    pull_policy: always
    ports:
      - 7860:7860
    restart: always
    environment:
      - GRADIO_SERVER_NAME=0.0.0.0
      - OLLAMA_HOST=ollama:11434
      - DB_HOST=127.0.0.1:80

  ###
  # The ollama LLM hosting service
  ###
  ollama:
    image: ollama/ollama:0.3.11
    ports:
      - 11434:11434
    restart: always
    environment:
      - OLLAMA_HOST=0.0.0.0
    volumes:
      - ollama_model_store:/root/.ollama # volume for persisting model downloads after container reboots, models can vary from 0...500GBs


volumes:
  ollama_model_store:
